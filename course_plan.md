# Dagskrá

## 1. Vika (13. janúar): Kynning á námskeiði (EH + KHK)

## 2. Vika (20. janúar): Birtingarskekkja (publication bias) (KHK)

This week examines how structural and methodological forces shape which scientific findings enter the published literature. We consider why statistically significant, novel, or expected results are more likely to be disseminated than null or contradictory findings, and how this selective visibility distorts the evidence base. The discussion highlights mechanisms such as selective reporting, analytical flexibility, and prestige-driven incentives, as well as empirical and statistical approaches for detecting biased patterns in published results. Together, these topics frame publication bias as a systemic issue that impacts inference, reproducibility, and the cumulative progress of psychological science.

### Lesefni: 

* Sterling, T. D. (1959). Publication Decisions and their Possible Effects on Inferences Drawn from Tests of Significance—or Vice Versa. Journal of the American Statistical Association, 54(285), 30–34. https://doi.org/10.1080/01621459.1959.10501497 
* OG: Sterling, T. D. (1995). Publication Decisions Revisited: The Effect of the Outcome of Statistical Tests on the Decision to Publish and Vice Versa. The American Statistician, 49(1), 108–112. https://doi.org/10.1080/00031305.1995.10476125
* Nosek, B. A., Spies, J., Motyl, M. (2012). Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058
* Ioannidis, J. P. A., Munafò, M. R., Fusar-Poli, P., Nosek, B. A., & David, S. P. (2014). Publication and other reporting biases in cognitive sciences: detection, prevalence, and prevention. Trends in Cognitive Sciences, 18(5), 235–241. https://doi.org/10.1016/j.tics.2014.02.010
* Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-Curve: A Key to the File-Drawer. Journal of Experimental Psychology: General, 143(2), 534–547 https://doi.org/10.1037/a0033242

## 3. Vika (27. janúar): Grunnatriði í Python (EH)

This week is a practical introduction to Python. Students use Python in a Jupyter notebook to learn how to load, inspect, clean, and visualize data with pandas and matplotlib. They will use the The OpenAlex Works + Authors Metadata Export dataset, which is a large, open publication-related dataset containing metadata on authors, journals, publication years, and related fields. The aim is for students to become comfortable navigating real datasets and performing exploratory data analysis before moving on to future tasks involving automation, replication, simulation, and inference. This week must be completed in Python.

## 4. Vika (3. febrúar): Scientific Misconduct, Data Fabrication, and Retractions (KHK)

Scientific misconduct represents an extreme but consequential breakdown in the research process. This week examines how fabricated data can enter the scientific record, the psychological and structural conditions that make such failures possible, and the systems designed to detect and correct them. We explore both cultural factors—such as incentives, oversight, and professional norms—and quantitative forensic techniques that reveal statistical irregularities in suspicious datasets. Retractions serve as a lens into how the research community responds to misconduct, what forms of correction are effective, and where current safeguards fall short. Understanding these dynamics clarifies the responsibilities of researchers, reviewers, and institutions in preserving the integrity of the scientific record.

### Lesefni:

* Chambers, C. (2017). The sin of corruptibility. In The Seven Deadly Sins of Psychology: A Manifesto for Reforming the Culture of Scientific Practice (pp. 96–125). Princeton University Press.
* Carlisle, J. B. (2017). Data fabrication and other reasons for non-random sampling in 5087 randomised, controlled trials in anaesthetic and general medical journals. Anaesthesia, 72(8), 944–952. https://doi.org/10.1111/anae.13938
* Brainard, J., &You, J. (2018, October 25). What a massive database of retracted papers reveals about science publishing’s ‘death penalty’: Better editorial oversight, not more flawed papers, might explain flood of retractions. Science. https://www.science.org/content/article/what-massive-database-retracted-papers-reveals-about-science-publishing-s-death-penalty 
* Simonsohn, U., Nelson, L., & Simmons, J. (2023, June 17). [109] Data Falsificada (Part 1): “Clusterfake”. Data Colada. https://datacolada.org/109

## 5. Vika (10. febrúar): Leit að óeðlilegum mynstrum í gögnum (EH)

Building on the data-handling and exploratory analysis experience from Week 3, this week introduces more autonomous work with data: crawling, scripting, and basic forensic analysis. Students may use either Python or R. They will scrape Kaggle or use its API to locate datasets suspected or rumored to contain fabricated or low-quality data, download the associated files, and perform simple forensic checks such as examining distributions, patterns, and other irregularities. The task then extends beyond the dataset itself: students write scripts to search for papers that have used these datasets, determine whether any of them have been retracted or questioned, and compare how the datasets are presented in the papers with how they appear in reality. The purpose is not only to detect signs of fabrication but also to develop general skills in crawling, metadata tracing, and verifying the scientific lineage of data.

## 6. Vika (17. febrúar): The Replication Crisis (KHK)

This week examines the emergence and evolution of the replication crisis in psychology, focusing on why replication became a central concern for the field and how scientific practices, statistical limitations, and theoretical ambiguity contributed to declining confidence in published findings. We explore the concepts of replication, reproducibility, and robustness, clarifying how they differ and how each relates to the credibility of empirical claims. The session includes an analysis of large-scale, multi-lab replication efforts that systematically evaluated the reliability of influential studies, as well as consideration of the scientific role of direct versus conceptual replications. Together, these discussions provide a comprehensive view of the factors that shape replicability in psychological science and the methodological and theoretical challenges involved in assessing whether findings genuinely hold.

### Lesefni:
* Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716
* Crandall, C. S., & Sherman, J. W. (2016). On the scientific superiority of conceptual replications for scientific progress. Journal of Experimental Social Psychology, 66, 93–99. https://doi.org/10.1016/j.jesp.2015.10.002
* Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T.-H., Huber, J., Johannesson, M., Kirchler, M., Nave, G., Nosek, B. A., Pfeiffer, T., Altmejd, A., Buttrick, N., Chan, T., Chen, Y., Forsell, E., Gampa, A., Heikensten, E., Hummer, L., Imai, T., … Wu, H. (2018). Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. Nature Human Behaviour, 2(9), 637–644. https://doi.org/10.1038/s41562-018-0399-z
Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., Hilgard, J., Struhl, M. K., Nuijten, M. B., Rohrer, J. M., Romero, F., Scheel, A. M., Scherer, L. D., Schönbrodt, F. D., & Vazire, S. (2022). Replicability, Robustness, and Reproducibility in Psychological Science. Annual Review of Psychology, 73, 719–748. https://doi.org/10.1146/annurev-psych-020821-114157

## 7. Vika (24. febrúar): Endurgerð birtra rannsókna (EH)

Following the conceptual overview of the replication crisis in Week 6, this week gives students a direct experience with replication. Students select one of three papers that rely on publicly available data. They may work in either Python or R. The assignment requires retrieving the dataset used in the original publication, reproducing the primary analyses as faithfully as possible, and documenting any ambiguous or missing methodological details. Students then compare their results with the published results and write a short replication report describing the level of agreement, any discrepancies, and their interpretation of what the replication does or does not show.

## 8. Vika (3. mars): Questionable Research and Measurement Practices (KHK)

This week examines how analytic and measurement flexibility can distort empirical conclusions in psychological research. We explore the mechanisms through which routine decisions—such as data exclusions, variable transformations, covariate choices, or outcome selection—inflate false-positive rates and obscure the true uncertainty in scientific findings. Attention is also given to measurement-related threats, including ambiguous construct definitions, insufficient validity evidence, and undisclosed scoring or modeling decisions. The session highlights how these vulnerabilities arise in both general research practice and psychometric modeling, illustrating how seemingly minor analytic or measurement choices can accumulate into substantial threats to reliability, replicability, and cumulative scientific progress.

### Lesefni:
* Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632
* Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4). https://doi.org/10.1177/2515245920952393
* Flores-Kanter, P. E., & Mosquera, M. (2023). How do you behave as a psychometrician? Research conduct in the context of psychometric research. The Spanish Journal of Psychology, 26, e13. https://doi.org/10.1017/SJP.2023.14
* Stefan, A. M., & Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. Royal Society Open Science, 10(2), 220346. https://doi.org/10.1098/rsos.220346

## 9. Vika (10. mars): Simulating questionable practices (EH)

Week 9 extends the discussion of week 8 into computational modeling, allowing students to explore how analytic flexibility affects results. Students may use Python or R. Students will simulate simple datasets under controlled conditions and investigate how different analysis choices (such as selective variable inclusion, outcome transformations, filtering decisions, or optional stopping) change the resulting p-values or effect sizes. Students build small interactive dashboards (using Shiny or Streamlit) to visualize how results shift under different analytic paths. The overarching aim is to turn the conceptual lessons from Week 8 into direct experience with how QRPs can generate seemingly meaningful findings even when the underlying effect is absent.

## 10. Vika (17. mars): NHST, Power, and Sample Size (KHK)

This week examines foundational issues in the way psychological science uses and misuses statistical inference. We explore how reliance on arbitrary significance thresholds (like p < .05), underpowered studies, and poor sample size justification contribute to false or unstable findings. The readings challenge the logic and utility of null hypothesis significance testing (NHST), question the reliability of much published research, and propose alternatives for more robust and transparent inferential practices. The goal is to critically reflect on how methodological conventions shape the evidentiary standards of psychology and what reforms are needed to make research findings more credible and cumulative.

### Lesefni:
* Cohen, J. (1994). The earth is round (p < .05). American Psychologist, 49(12), 997–1003. https://doi.org/10.1037/0003-066X.49.12.997
* Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLOS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124
* McShane, B. B., Gal, D., Gelman, A., Robert, C., & Tackett, J. L. (2019). Abandon Statistical Significance. The American Statistician, 73(sup1), 235–245. https://doi.org/10.1080/00031305.2018.1527253
* Lakens, D. (2022). Sample Size Justification. Collabra: Psychology, 8(1), 33267. https://doi.org/10.1525/collabra.33267

Ítarefni:
Dance of p values (video): https://www.youtube.com/watch?v=ez4DgdurRPg
Cowles, M., & Davis, C. (1982). On the origins of the .05 level of statistical significance. American Psychologist, 37(5), 553–558. https://doi.org/10.1037/0003-066X.37.5.553

## 11. Vika (24. mars): Bayesísk ályktunarfræði (EH)

Week 11 introduces Bayesian inference as an alternative way of quantifying uncertainty. Students may work in Python or R. They will design a simple generative model, simulate data from known parameters, and fit a Bayesian model to recover those parameters. By comparing the true values with the posterior distributions, students gain an intuitive sense of what Bayesian credible intervals represent and how Bayesian reasoning addresses issues discussed in Week 10, particularly uncertainty quantification and overreliance on binary significance decisions. 

## 12. Vika (31. mars): Preregistration and the Future of Scientific Reform (KHK)
This week focuses on preregistration as a core transparency reform, examining both its potential to strengthen confirmatory inference and the concerns raised about its misuse and incentive-driven distortions. Through an ongoing scholarly debate, we consider what preregistration can achieve, where it falls short, and how it fits into broader efforts to promote more robust and trustworthy psychological science.

### Lesefni:

* Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114
* Klonsky, E. D. (2025). Campbell’s Law Explains the Replication Crisis: Pre-Registration Badges Are History Repeating. Assessment, 32(2), 224–234. https://doi.org/10.1177/10731911241253430
* Vize, C. E., Phillips, N. L., Miller, J. D., & Lynam, D. R. (2025). On the Use and Misuses of Preregistration: A Reply to Klonsky (2024). Assessment, 32(2), 235–243. https://doi.org/10.1177/10731911241275256
* Klonsky, E. D. (2025). How to Produce, Identify, and Motivate Robust Psychological Science: A Roadmap and a Response to Vize et al. Assessment, 32(2), 244–252. https://doi.org/10.1177/10731911241299723

## PÁSKAFRÍ (7. apríl)

## 13. Vika (14. apríl): Inngangur að vitvélum (EH)

This final technical week stands on its own and introduces students to a simple end-to-end machine learning workflow using Python. The week draws on selected introductory chapters from Murphy and Géron. Students load a dataset, prepare it for modeling, train a basic predictive model, evaluate the model’s performance, and reflect on the relationship between traditional statistical inference and machine learning approaches. The aim is not to teach machine learning comprehensively but to give students a coherent and accessible first experience with modern predictive modeling.
